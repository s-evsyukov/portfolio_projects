# Data engineer portfolio

---

### Pet-projects Description:

| Project name                               | Description                                                                                                                                          | 	Skills and tools                                                                                                           |
|:-------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------|
| [*Data warehouse*][1]                      | Design `DWH` and describe the subject area                                                                                                           | `Inmon` `Kimball` `Data volt2` `Anchor modeling`                                                                            |
| [*Hadoop HDFS map reduce*][2]              | MapReduce task for aggregating data about New York Taxi using `Hadoop HDFS` infrastructure                                                           | `Yandex.Cloud` `HadoopStreaming` `HDFS` `S3` `MapReduce` `CLI` `Shell` `Hadoop Cluster Administration` `ETL`                |
| [*Hadoop Hive*][3]                         | Providing constant access to cold data, creating a Star scheme and a showcase using the `Hadoop Hive` infrastructure                                 | `Yandex.Cloud` `S3` `HDFS` `HIVE` `MapReduce` `TEZ` `YARN` `HiveSQL` `CLI` `Shell` `Hadoop Cluster Administration`          |
| [*Apache Spark*][4]                        | Creating a data showcase using the `Apache Spark` infrastructure                                                                                     | `Yandex.Cloud` `S3` `HDFS` `PySpark` `CLI` `Shell` `Hadoop Cluster Administration`                                          |
| [*Docker Kafka Spark*][5]                  | Creating a data showcase using the `Docker-compose`, `Kafka`, `GreenPlum` and `Spark` infrastructure                                                 | `Yandex.Cloud` `Kafka` `SparkStreaming` `Docker-compose` `ZooKeeper` `GreenPlum`                                            |
| [*Apache Airflow*][6]                      | Auto-collecting of currency exchange rate data from the website with `Apache Kafka` and uploading to `GreenPlum`                                     | `VK.Cloud` `Airflow` `GreenPlum` `Jinja` `macros` `ETL` `parsing` `bash` `IDE` `CI/CD`                                      |
| [*Google Kubernetes*][7]                   | Deploying a `Kubernetes` cluster with the installation of components to run the custom script and tracking the result using `Spring History Server`  | `VK.Cloud` `terminal` `Ubuntu` `Kubectl` `Kubernetes` `Helm` `DOCKER` `S3` `Spark` `Spark Operator`  `Spark History Server` |
| [*Apache SparkML*][8]                      | Creating a bot identifier using `PySpark` among user sessions with two tasks - to train the best data model and to apply it.                         | `PySpark` `SparkML` `SparkSession` `Pipeline`                                                                               |
| [*Docker PostgreSQL*][9]                   | Initiating `PostgreSQL` container with `Docker-compose`                                                                                              | `Docker` `Docker-compose` `PostgreSQL` `Adminer` `Python` `Pscorpg2` `DockerHUB`                                            |
| [*Docker Debezium Kafka PostgreSQL*][10]   | Creating `Kafka` topics monitoring pipeline with `Debezium connect`                                                                                  | `Docker` `Docker-compose` `PostgreSQL` `Debezium` `Kafka`                                                                   |
| [*PySpark Poetry*][11]                     | Creating `PySpark` project with `Poetry` DMS                                                                                                         | `PySpark` `Poetry` `PyTest` `Quinn` `Wheel`                                                                                 |
| [*Docker Spark cluster*][12]               | Creating standalone `Spark` cluster on local PC                                                                                                      | `Docker` `Docker-compose` `PySpark` `AWS CLI` `PostgreSQL` `Terminal` `Bash`                                                |
| [*Docker Airflow Hive HDFS Spark*][13]     | Pet project with with `Apache Airflow` `PySpark` pipeline to `ETL` Forex data                                                                        | `Docker` `Docker-compose` `Airflow` `HDFS` `Hive` `PySpark` `Bash`                                                          |                                              

[1]:https://github.com/Amboss/data_warehouse
[2]:https://github.com/Amboss/hadoop_mapreduce
[3]:https://github.com/Amboss/hadoop_hive
[4]:https://github.com/Amboss/apache_spark
[5]:https://github.com/Amboss/docker_kafka_spark
[6]:https://github.com/Amboss/apache_airflow
[7]:https://github.com/Amboss/google_kubernetes
[8]:https://github.com/Amboss/apache_pyspark_ml
[9]:https://github.com/Amboss/docker_postgres_python
[10]:https://github.com/Amboss/Postgres_Debezium_Kafka
[11]:https://github.com/Amboss/Spark_Poetry
[12]:https://github.com/Amboss/Docker_Spark_cluster
[13]:https://github.com/s-evsyukov/portfolio_projects/tree/master/Docker_AF_hive_spark
